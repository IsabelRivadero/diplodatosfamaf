{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copia de 2_data_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF6fBS3jMCwn",
        "colab_type": "text"
      },
      "source": [
        "# Procesamiento de datos usando Tensorflow\n",
        "\n",
        "Cuando trabajamos con Tensorflow, existen una gran variedad de formas en las que podemos alimentar los datos a nuestra red neuronal. Esto también tiene que ver con el tipo de datos y los pasos de pre-procesamiento que sean necesarios.\n",
        "\n",
        "En la notebook 1 se utilizó un conjunto de datos de imágenes, esencialmente con variables numéricas. En esta notebook trabajemos con datos categóricos y profundizaremos en cómo trasformar los ejemplos dentro del pipeline de clasificación.\n",
        "\n",
        "Ante un problema de clasificación, lo primero que debemos hacer es **inspeccionar los datos y construir un prototipo de modelo**. La forma más fácil de hacerlo es con notebooks. Sin embargo, a la hora de llevar a cabo experimentos con redes neuronales, un entorno interactivo puede no ser la mejor opción. En primer lugar, explorar los hiperparámetros de una arquitectura neuronal puede llevar varias horas e incluso días, perdiendo todas las ventajas del entorno interactivo. En segundo lugar, no podemos encolar ejecuciones de notebooks para reservar recursos como las GPUs.\n",
        "\n",
        "Por ello, primero realizaremos una exploración inicial de los datos en esta notebook. Una vez que decidamos qué tipo de modelo implementar, pasaremos el modelo a un script de python que cargue los datos, construya el modelo, lo entrene, y finalmente guarde las métricas relevantes.\n",
        "\n",
        "En esta notebook, veremos varios conceptos avanzados de entrenamiento de redes:\n",
        "\n",
        "  * Uso de `tf.data.Dataset` para optimizar la ingesta de datos. \n",
        "  * Uso de capas `tf.layers.Embedding`.\n",
        "  * Combinación de distintos tipos de features en un mismo modelo con múltiples inputs.\n",
        "  * MLFlow para registro de experimentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tntC5azgMCwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy\n",
        "import pandas\n",
        "import seaborn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "seaborn.set_style('whitegrid')\n",
        "seaborn.set_palette('colorblind')\n",
        "seaborn.set_context('paper')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffBOB4zcMCws",
        "colab_type": "text"
      },
      "source": [
        "## Cargando los datos\n",
        "\n",
        "Una vez más, estaremos trabajando con el conjunto de datos `petfinder`. Deben descargarlo siguiendo las instrucciones en la [notebook 0](./0_set_up.ipynb), descomprimirlo y luego ajustar la dirección en esta notebook según corresponda. \n",
        "\n",
        "Algunas de las preguntas que respondemos durante esta etapa son:\n",
        "\n",
        " * ¿Qué tipo de tarea tengo que resolver? ¿Clasificación o regresión?\n",
        " * ¿Qué distribución tienen mis etiquetas?\n",
        " * ¿Qué tipo de datos tengo disponible para la clasificación? ¿Cuáles son útiles?\n",
        " * Dadas las características disponibles y el problema que quiero resolver, ¿qué tipo de clasificador o arquitectura conviene utilizar? ¿De qué manera se están representando las causas latentes del problema en el modelo elegido?\n",
        " * Dadas las características disponibles y el modelo elegido, ¿de qué forma representaremos cada una de dichas características?\n",
        " \n",
        "En esta clase utilizaremos redes neuronales como modelos porque es el objetivo de la materia, pero sigue siendo importante qué aspectos podremos capturar con este tipo de modelo, especialmente para tener intuiciones sobre qué hiperparámetros explorar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjiE7nwMCwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIRECTORY = '/content/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9ocFpIZMCwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a sample of data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset, dev_dataset = train_test_split(\n",
        "    pandas.read_csv(os.path.join(DATA_DIRECTORY, 'train.csv')), test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elXz40w3MCw0",
        "colab_type": "code",
        "outputId": "af4ff8b0-be77-40df-b684-08da5fffa74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "dataset[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Age</th>\n",
              "      <th>Breed1</th>\n",
              "      <th>Breed2</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Color1</th>\n",
              "      <th>Color2</th>\n",
              "      <th>Color3</th>\n",
              "      <th>MaturitySize</th>\n",
              "      <th>FurLength</th>\n",
              "      <th>Vaccinated</th>\n",
              "      <th>Dewormed</th>\n",
              "      <th>Sterilized</th>\n",
              "      <th>Health</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Fee</th>\n",
              "      <th>State</th>\n",
              "      <th>Description</th>\n",
              "      <th>AdoptionSpeed</th>\n",
              "      <th>PID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7161</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>I was abandoned at R&amp;R Serdang by my cruel and...</td>\n",
              "      <td>4</td>\n",
              "      <td>10073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>103</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>Lady was found lost in Taman Tun. She is a fri...</td>\n",
              "      <td>2</td>\n",
              "      <td>7854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>266</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>4 pretty little kitties orphaned and rescued f...</td>\n",
              "      <td>1</td>\n",
              "      <td>1618</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Type  Age  ...  AdoptionSpeed    PID\n",
              "7161     1    4  ...              4  10073\n",
              "5568     1   36  ...              2   7854\n",
              "1140     2    1  ...              1   1618\n",
              "\n",
              "[3 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLOHv9OJMCw5",
        "colab_type": "text"
      },
      "source": [
        "### Tipos de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-elPyIYMCw6",
        "colab_type": "code",
        "outputId": "7297c463-f30c-48c6-a7f0-c8aedf435bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "dataset.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Type              int64\n",
              "Age               int64\n",
              "Breed1            int64\n",
              "Breed2            int64\n",
              "Gender            int64\n",
              "Color1            int64\n",
              "Color2            int64\n",
              "Color3            int64\n",
              "MaturitySize      int64\n",
              "FurLength         int64\n",
              "Vaccinated        int64\n",
              "Dewormed          int64\n",
              "Sterilized        int64\n",
              "Health            int64\n",
              "Quantity          int64\n",
              "Fee               int64\n",
              "State             int64\n",
              "Description      object\n",
              "AdoptionSpeed     int64\n",
              "PID               int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJyLzuYBMCw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_col = 'AdoptionSpeed'\n",
        "nlabels = dataset[target_col].unique().shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxCfGv_SMCxB",
        "colab_type": "text"
      },
      "source": [
        "### Distribución de las etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHFDKMXxMCxC",
        "colab_type": "code",
        "outputId": "e86cbd36-9b4e-485f-cb1d-fc565d7f5033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "seaborn.countplot(dataset.AdoptionSpeed, color='blue')\n",
        "seaborn.despine()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEHCAYAAABbZ7oVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGP0fFO44ArbXq1wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbdt-PwwMCxF",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\n",
        "\n",
        "https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
        "\n",
        "Why not to use feature_columns https://github.com/tensorflow/tensorflow/issues/27895\n",
        "\n",
        "feature_columns doc 2.0 https://www.tensorflow.org/api_docs/python/tf/feature_column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3ssW5X-MCxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g1s4PHLMCxJ",
        "colab_type": "text"
      },
      "source": [
        "## Creando las representaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNvqn8v4MCxK",
        "colab_type": "text"
      },
      "source": [
        "Tenemos una serie de variables categóricas y ordinales que pueden ser útiles para predecir la velocidad de adopción. Para cada una de ellas, tenemos que pensar cuál es la mejor forma de pasarla como input a la red. Analizaremos algunas de ellas:\n",
        "\n",
        "  * `Age` es una variable numérica discreta, podemos representarla con una única neurona con el valor original. Es muy importante normalizar este tipo de variables.\n",
        "  * `Gender` es una variable categórica. Como la variable tiene pocos valores, utilizaremos un *one-hot encoding* como representación.\n",
        "  * `Breed1` es una variable categórica que puede tomar muchos valores. Podemos utilizar *one-hot encoding*, lo cual resultará en vectores esparsos de dimensión cercana a 300. Alternativamente, podemos utilizar una capa de embedding para representar sus valores con un vector denso de baja dimesionalidad. Pregunta: ¿qué información podrá capturar este embedding?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1suB1mfUMCxL",
        "colab_type": "text"
      },
      "source": [
        "Una vez que definimos cómo vamos a representar cada una de las columnas, las pre-procesamos para formar un numpy array. En este caso, procesaremos el dataset completo porque estamos seguros de que entrará en memoria. En otros casos, puede ser necesario un pre-procesamiento por batches, o incluso utilizar las funciones de Tensorflow incluidas en el módulo `feature_column`.\n",
        "\n",
        "NOTA: para este ejercicio, intentamos utilizar `feature_column` pero causaba que la loss diverga. La documentación no ha sido totalmente actualizada a Tensorflow 2.0, y puede ser que nos encontremos ante un error de cambio de versiones. Pueden encontrar más ejemplos en [este link](https://www.tensorflow.org/tutorials/structured_data/feature_columns)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0IZK8aXMCxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It's important to always use the same one-hot length\n",
        "one_hot_columns = {\n",
        "    one_hot_col: dataset[one_hot_col].max()\n",
        "    for one_hot_col in ['Gender', 'Color1','Color2','Color3','Sterilized','Health','Type','MaturitySize'] # aca vemos el valor maximo que hay en la columna Gender y Color1,\n",
        "}\n",
        "embedded_columns = {\n",
        "    embedded_col: dataset[embedded_col].max() + 1\n",
        "    for embedded_col in ['Breed1','Breed2'] # aca vemos el valor maximo que hay en la columna Breed1 +1\n",
        "}\n",
        "numeric_columns = ['Age', 'Fee']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gm_W_EOMCxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_features(df):\n",
        "    direct_features = []\n",
        "\n",
        "    # Create one hot encodings\n",
        "    for one_hot_col, max_value in one_hot_columns.items():\n",
        "        direct_features.append(tf.keras.utils.to_categorical(df[one_hot_col] - 1, max_value)) # crea una matris de 1's y 0's con tantas columnas como clases\n",
        "\n",
        "\n",
        "        \n",
        "    # Concatenate all features that don't need further embedding into a single matrix.\n",
        "    features = {'direct_features': numpy.hstack(direct_features)} #pone todos los valores de forma de horizontal\n",
        "\n",
        "    # Create embedding columns - nothing to do here. We will use the zero embedding for OOV\n",
        "    for embedded_col in embedded_columns.keys():\n",
        "        features[embedded_col] = df[embedded_col].values\n",
        "\n",
        "    # Create and append numeric columns\n",
        "    # Don't forget to normalize!\n",
        "    for i in numeric_columns:\n",
        "        featu_nor = MinMaxScaler().fit_transform([df[i].values])\n",
        "        direct_features.append(featu_nor)\n",
        "\n",
        "    # Convert labels to one-hot encodings\n",
        "    targets = tf.keras.utils.to_categorical(df[target_col], nlabels)\n",
        "    \n",
        "    return features, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1xnvx7SMCxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = process_features(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg8EQfz3MCxY",
        "colab_type": "code",
        "outputId": "e6c75925-c4e2-4b30-e5a1-4dab8b0f5a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc9NsopzA_uo",
        "colab_type": "code",
        "outputId": "49470211-68f3-4d4c-f158-612690cc44ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Breed1': array([307, 103, 266, ..., 266, 266, 307]),\n",
              " 'Breed2': array([  0,   0,   0, ..., 265,   0,   0]),\n",
              " 'direct_features': array([[0., 1., 0., ..., 1., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 1.],\n",
              "        [0., 0., 1., ..., 1., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 1., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 1., 0., 0.]], dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odjdtGPtMCxb",
        "colab_type": "code",
        "outputId": "d1cac3d4-325d-4139-e730-a2107ef60eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "direct_features_input_shape = (X_train['direct_features'].shape[1],)\n",
        "direct_features_input_shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLPoqL2mMCxe",
        "colab_type": "text"
      },
      "source": [
        "## Creando datasets iterables\n",
        "\n",
        "Como hemos visto, las redes neuronales se entrenan iterativamente con el algoritmo de *stochastic gradient descent*. Una forma de hacerlo es pasarle el dataset entero al método `fit` de un modelo de Keras, como vimos en la notebook anterior. Sin embargo, esto tiene algunas desventajas:\n",
        "\n",
        "* El dataset procesado debe entrar en memoria\n",
        "* El dataset procesado debe entrar en disco, lo cual no siempre es factible para encodings y datasets realmente grandes (ej: la wikipedia)\n",
        "* Una vez que la GPU ha terminado de procesar los datos, devuelve el control a la CPU (que estaba esperando sin hacer nada), y espera a que los nuevos datos son particionados.\n",
        "* No es posible usar cálculo distribuido en distintos file systems.\n",
        "\n",
        "Las dos primeras desventajas se solucionan preprocesando los datos en batches, y creando matrices anchas pero con pocas filas. Sin embargo, escribir este código manualmente puede ser complejo y en general lo hacemos de manera ineficiente. Solucionar las dos últimas es bastante más complicado y a la vez crítico. \n",
        "\n",
        "> **No importa qué tan buen hardware usemos para el entrenamiento del modelo, si seguimos limitados por un procesamiento de datos lineal y single core.**\n",
        "\n",
        "Por eso es recomendable utilizar las abstracciones nativas provistas por Tensorflow que paralelizan internamente muchas funciones.\n",
        "\n",
        "Para ello, crearemos un objeto `tf.data.Dataset` iterable a partir de nuestro dataframe de pandas y no tendremos que preocuparnos por la optimización de la GPU. Los datasets saben cómo crear batches, shuffles, aplicar funciones map y filter, etc. Además, podemos crear datasets a partir de diversas estructuras de datos, como numpy arrays o archivos. Pueden encontrar más información sobre los distintos tipos de Datasets en [este tutorial](https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BS3edQ0MCxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "# TODO shuffle the train dataset!\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    process_features(dev_dataset)).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ul-ZCzsMCxh",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver qué es lo que tiene adentro en dataset obteniendo la primera operación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujt7lHB8MCxi",
        "colab_type": "code",
        "outputId": "252bb688-0a05-471a-ad60-38bdb4ecf885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "x_batch, y_batch = next(iter(train_ds))\n",
        "x_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Breed1': <tf.Tensor: id=403460, shape=(32,), dtype=int64, numpy=\n",
              "  array([307, 103, 266, 152, 266, 266, 292, 307, 307, 307, 251, 109, 307,\n",
              "         266, 276, 266,  50, 266, 307, 307, 307, 266, 307, 307, 307, 307,\n",
              "         307, 307, 307, 307, 307, 307])>,\n",
              "  'Breed2': <tf.Tensor: id=403461, shape=(32,), dtype=int64, numpy=\n",
              "  array([  0,   0,   0, 307, 266,   0,   0, 307,   0,   0, 254, 307, 307,\n",
              "         266, 292,   0,   0,   0,   0, 128, 307,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0, 307,   0,   0])>,\n",
              "  'direct_features': <tf.Tensor: id=403462, shape=(32, 36), dtype=float32, numpy=\n",
              "  array([[0., 1., 0., ..., 1., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 1.],\n",
              "         [0., 0., 1., ..., 1., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 1., ..., 1., 0., 0.],\n",
              "         [0., 1., 0., ..., 1., 0., 0.]], dtype=float32)>},\n",
              " <tf.Tensor: id=403463, shape=(32, 5), dtype=float32, numpy=\n",
              " array([[0., 0., 0., 0., 1.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1., 0.]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MQVUSy8MCxl",
        "colab_type": "text"
      },
      "source": [
        "## Construyendo el modelo\n",
        "\n",
        "Construimos el modelo, por ahora con sólo una capa oculta. Sin embargo, la complejidad más grande es combinar los features que tienen embeddings con los que no. Por cada tipo de feature, tenemos que agregar una capa de `Input`. Tener en cuenta que cada embedded feature se considera distinto.\n",
        "\n",
        "Como tenemos más de un input, tenemos que usar la API funcional de Keras en lugar de usar un modelo `Sequential`. La API funcional puede construir modelos más flexibles, ya que conectaremos explícitamente cada capa con su capa siguiente.\n",
        "\n",
        "Pueden encontrar otro ejemplo similar a este código en [esta notebook](https://www.kaggle.com/alexanderkireev/deep-learning-support-9663)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYuu2j64MCxm",
        "colab_type": "code",
        "outputId": "c844b769-ad5d-4f14-c8e2-ce964b61bf86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "hidden_layer_size = 32\n",
        "\n",
        "# Add one input and one embedding for each embedded column\n",
        "embedding_layers = []\n",
        "inputs = []\n",
        "for embedded_col, max_value in embedded_columns.items():\n",
        "    input_layer = layers.Input(shape=(1,), name=embedded_col)\n",
        "    inputs.append(input_layer)\n",
        "    # Define the embedding layer\n",
        "    embedding_size = int(max_value / 4)\n",
        "    embedding_layers.append(\n",
        "        tf.squeeze(layers.Embedding(input_dim=max_value, output_dim=embedding_size)(input_layer), axis=-2))\n",
        "    print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
        "\n",
        "# Add the direct features already calculated\n",
        "direct_features_input = layers.Input(shape=direct_features_input_shape, name='direct_features')\n",
        "inputs.append(direct_features_input)\n",
        "    \n",
        "# Concatenate everything together\n",
        "features = layers.concatenate(embedding_layers + [direct_features_input])\n",
        "\n",
        "dense1 = layers.Dense(hidden_layer_size, activation='relu')(features)\n",
        "output_layer = layers.Dense(nlabels, activation='softmax')(dense1)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=output_layer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding embedding of size 77 for layer Breed1\n",
            "Adding embedding of size 77 for layer Breed2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9PgPLKzMCxp",
        "colab_type": "text"
      },
      "source": [
        "### Métricas de evaluación\n",
        "\n",
        "Al igual que en la materia de aprendizaje supervisado, utilizaremos el accuracy como métrica, y agregaremos el score f1. Es opcional implementar esta predicción como un problema de regresión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6GBXCj3MCxp",
        "colab_type": "code",
        "outputId": "9c1ca129-967a-450f-9871-d9ccf1c17c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Breed2 (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 77)        23716       Breed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1, 77)        23716       Breed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze (TensorFlow [(None, 77)]         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_1 (TensorFl [(None, 77)]         0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "direct_features (InputLayer)    [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 190)          0           tf_op_layer_Squeeze[0][0]        \n",
            "                                                                 tf_op_layer_Squeeze_1[0][0]      \n",
            "                                                                 direct_features[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           6112        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            165         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 53,709\n",
            "Trainable params: 53,709\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBKJkw1tMCxu",
        "colab_type": "text"
      },
      "source": [
        "## Entrenando el modelo\n",
        "\n",
        "Una vez que tenemos definido nuestro modelo, tenemos que entrenarlo. Sin embargo, para que los resultados sean útiles, tenemos que llevar un registro de qué hiperparámetros utilizamos y qué performance obtuvimos. Para eso, usaremos [MLFlow](https://mlflow.org/docs/latest/quickstart.html), una librería muy simple pero que permite sistematizar el registro de resultados.\n",
        "\n",
        "MLFlow soporta muchísimos casos de uso, pero por ahora sólo usaremos el más básico de todos para organizar el entrenamiento. Llamaremos *experiments* a los cambios grandes en la arquitectura, por ejemplo, si agregamos muchas capas nuevas o mecanismos de regularización. Llamaremos *runs* a las distintas ejecuciones de la misma arquitectura donde variamos sólo algunos hiperparámetros, como funciones de activación, cantidad de neuronas, tamaños de los embeddings, etc.\n",
        "\n",
        "Para acceder a la interfaz gráfica donde podemos ver las *run*, en una nueva terminal tenemos que ejecutar \n",
        "\n",
        "    $ mlflow ui -p PORT\n",
        "    \n",
        "Y abrir `https://localhost:PORT` en nuestro navegador (donde `PORT` es un número de puerto). Si estamos en un servidor, es probab, tendremos que abrir un nuevo puerto ssh a `PORT`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D8JaQRsG8mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! pip install mlflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JToc0xH0MCxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mlflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "616-MH5eMCxy",
        "colab_type": "code",
        "outputId": "d7de1100-b9b7-46c6-f19c-45735640712a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mlflow.set_experiment('very_base_approach')\n",
        "\n",
        "with mlflow.start_run(nested=True):\n",
        "    # Log model hiperparameters first\n",
        "    mlflow.log_param('hidden_layer_size', hidden_layer_size)\n",
        "    mlflow.log_param('embedded_columns', embedded_columns)\n",
        "    mlflow.log_param('one_hot_columns', one_hot_columns)\n",
        "    #mlflow.log_param('numerical_columns', numerical_columns)  # Not using these yet\n",
        "    \n",
        "    # Train\n",
        "    epochs = 100\n",
        "    history = model.fit(train_ds, epochs=epochs)\n",
        "    \n",
        "    # Evaluate\n",
        "    loss, accuracy = model.evaluate(test_ds)\n",
        "    print(\"*** Test loss: {} - accuracy: {}\".format(loss, accuracy))\n",
        "    mlflow.log_metric('epochs', epochs)\n",
        "    mlflow.log_metric('loss', loss)\n",
        "    mlflow.log_metric('accuracy', accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "265/265 [==============================] - 2s 8ms/step - loss: 1.4600 - accuracy: 0.3068\n",
            "Epoch 2/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.4054 - accuracy: 0.3523\n",
            "Epoch 3/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3898 - accuracy: 0.3665\n",
            "Epoch 4/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3784 - accuracy: 0.3770\n",
            "Epoch 5/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3683 - accuracy: 0.3824\n",
            "Epoch 6/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3588 - accuracy: 0.3898\n",
            "Epoch 7/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3502 - accuracy: 0.3924\n",
            "Epoch 8/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3421 - accuracy: 0.3934\n",
            "Epoch 9/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3346 - accuracy: 0.3969\n",
            "Epoch 10/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3274 - accuracy: 0.4005\n",
            "Epoch 11/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3209 - accuracy: 0.4040\n",
            "Epoch 12/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3147 - accuracy: 0.4071\n",
            "Epoch 13/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3089 - accuracy: 0.4098\n",
            "Epoch 14/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.3035 - accuracy: 0.4144\n",
            "Epoch 15/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2980 - accuracy: 0.4175\n",
            "Epoch 16/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2931 - accuracy: 0.4204\n",
            "Epoch 17/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2884 - accuracy: 0.4247\n",
            "Epoch 18/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2842 - accuracy: 0.4279\n",
            "Epoch 19/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2803 - accuracy: 0.4306\n",
            "Epoch 20/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2766 - accuracy: 0.4328\n",
            "Epoch 21/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2731 - accuracy: 0.4360\n",
            "Epoch 22/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2697 - accuracy: 0.4357\n",
            "Epoch 23/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2664 - accuracy: 0.4371\n",
            "Epoch 24/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2635 - accuracy: 0.4378\n",
            "Epoch 25/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2605 - accuracy: 0.4410\n",
            "Epoch 26/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2573 - accuracy: 0.4430\n",
            "Epoch 27/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2548 - accuracy: 0.4447\n",
            "Epoch 28/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2522 - accuracy: 0.4445\n",
            "Epoch 29/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2496 - accuracy: 0.4445\n",
            "Epoch 30/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2473 - accuracy: 0.4463\n",
            "Epoch 31/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2449 - accuracy: 0.4468\n",
            "Epoch 32/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2424 - accuracy: 0.4488\n",
            "Epoch 33/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2403 - accuracy: 0.4496\n",
            "Epoch 34/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2383 - accuracy: 0.4507\n",
            "Epoch 35/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2360 - accuracy: 0.4515\n",
            "Epoch 36/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2341 - accuracy: 0.4528\n",
            "Epoch 37/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2326 - accuracy: 0.4542\n",
            "Epoch 38/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2306 - accuracy: 0.4561\n",
            "Epoch 39/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2290 - accuracy: 0.4566\n",
            "Epoch 40/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2273 - accuracy: 0.4574\n",
            "Epoch 41/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2257 - accuracy: 0.4575\n",
            "Epoch 42/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2244 - accuracy: 0.4584\n",
            "Epoch 43/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2229 - accuracy: 0.4585\n",
            "Epoch 44/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2213 - accuracy: 0.4588\n",
            "Epoch 45/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2199 - accuracy: 0.4592\n",
            "Epoch 46/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2183 - accuracy: 0.4615\n",
            "Epoch 47/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2168 - accuracy: 0.4612\n",
            "Epoch 48/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2153 - accuracy: 0.4639\n",
            "Epoch 49/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2139 - accuracy: 0.4649\n",
            "Epoch 50/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2128 - accuracy: 0.4647\n",
            "Epoch 51/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2114 - accuracy: 0.4659\n",
            "Epoch 52/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.2100 - accuracy: 0.4664\n",
            "Epoch 53/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2088 - accuracy: 0.4662\n",
            "Epoch 54/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2077 - accuracy: 0.4666\n",
            "Epoch 55/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2064 - accuracy: 0.4676\n",
            "Epoch 56/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2050 - accuracy: 0.4672\n",
            "Epoch 57/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2040 - accuracy: 0.4670\n",
            "Epoch 58/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2030 - accuracy: 0.4696\n",
            "Epoch 59/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2019 - accuracy: 0.4709\n",
            "Epoch 60/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.2009 - accuracy: 0.4704\n",
            "Epoch 61/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.1998 - accuracy: 0.4706\n",
            "Epoch 62/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1987 - accuracy: 0.4717\n",
            "Epoch 63/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1978 - accuracy: 0.4730\n",
            "Epoch 64/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1969 - accuracy: 0.4738\n",
            "Epoch 65/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1960 - accuracy: 0.4735\n",
            "Epoch 66/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1946 - accuracy: 0.4749\n",
            "Epoch 67/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1938 - accuracy: 0.4762\n",
            "Epoch 68/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1930 - accuracy: 0.4741\n",
            "Epoch 69/100\n",
            "265/265 [==============================] - 1s 3ms/step - loss: 1.1918 - accuracy: 0.4757\n",
            "Epoch 70/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1912 - accuracy: 0.4771\n",
            "Epoch 71/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1905 - accuracy: 0.4756\n",
            "Epoch 72/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1895 - accuracy: 0.4757\n",
            "Epoch 73/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1887 - accuracy: 0.4767\n",
            "Epoch 74/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1879 - accuracy: 0.4788\n",
            "Epoch 75/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1870 - accuracy: 0.4774\n",
            "Epoch 76/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1865 - accuracy: 0.4784\n",
            "Epoch 77/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1857 - accuracy: 0.4793\n",
            "Epoch 78/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1847 - accuracy: 0.4775\n",
            "Epoch 79/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1842 - accuracy: 0.4769\n",
            "Epoch 80/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1830 - accuracy: 0.4770\n",
            "Epoch 81/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1828 - accuracy: 0.4787\n",
            "Epoch 82/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1820 - accuracy: 0.4777\n",
            "Epoch 83/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1815 - accuracy: 0.4763\n",
            "Epoch 84/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1808 - accuracy: 0.4761\n",
            "Epoch 85/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1802 - accuracy: 0.4758\n",
            "Epoch 86/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1800 - accuracy: 0.4754\n",
            "Epoch 87/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1795 - accuracy: 0.4775\n",
            "Epoch 88/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1787 - accuracy: 0.4782\n",
            "Epoch 89/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1782 - accuracy: 0.4770\n",
            "Epoch 90/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1776 - accuracy: 0.4771\n",
            "Epoch 91/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1773 - accuracy: 0.4780\n",
            "Epoch 92/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1770 - accuracy: 0.4777\n",
            "Epoch 93/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1763 - accuracy: 0.4775\n",
            "Epoch 94/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1758 - accuracy: 0.4774\n",
            "Epoch 95/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1753 - accuracy: 0.4794\n",
            "Epoch 96/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1751 - accuracy: 0.4786\n",
            "Epoch 97/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1745 - accuracy: 0.4794\n",
            "Epoch 98/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1738 - accuracy: 0.4787\n",
            "Epoch 99/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1736 - accuracy: 0.4800\n",
            "Epoch 100/100\n",
            "265/265 [==============================] - 1s 2ms/step - loss: 1.1733 - accuracy: 0.4799\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 1.7691 - accuracy: 0.3321\n",
            "*** Test loss: 1.7691300360124502 - accuracy: 0.3320736885070801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu4tNmIDMCx2",
        "colab_type": "text"
      },
      "source": [
        "## Evaluando del modelo\n",
        "\n",
        "Además de tener en cuenta las métricas de performance del modelo, es importante mirar los resultados obtenidos y controlar que el modelo efectivamente está aprendiendo algo relevante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCC0-O6YMCx2",
        "colab_type": "code",
        "outputId": "06dfed99-62ae-4f6e-9382-ee908e2259dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "predictions = numpy.argmax(model.predict(test_ds), axis=1)\n",
        "seaborn.countplot(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9c7c6aff60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD5CAYAAAAndkJ4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwEIhAAAkSf8LYte5E7GcTJ8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kybPD411FHlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "con = pd.DataFrame(np.arange(1,2118),columns=['PID'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTtuwJEcfNGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "con['AdoptionSpeed'] = predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EdVVosdfPvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_features2(df):\n",
        "    direct_features = []\n",
        "\n",
        "    # Create one hot encodings\n",
        "    for one_hot_col, max_value in one_hot_columns.items():\n",
        "        direct_features.append(tf.keras.utils.to_categorical(df[one_hot_col] - 1, max_value)) # crea una matris de 1's y 0's con tantas columnas como clases\n",
        "\n",
        "    # Create and append numeric columns\n",
        "    # Don't forget to normalize!\n",
        "    # ....\n",
        "    \n",
        "    # Concatenate all features that don't need further embedding into a single matrix.\n",
        "    features = {'direct_features': numpy.hstack(direct_features)} #pone todos los valores de forma de horizontal\n",
        "\n",
        "    # Create embedding columns - nothing to do here. We will use the zero embedding for OOV\n",
        "    for embedded_col in embedded_columns.keys():\n",
        "        features[embedded_col] = df[embedded_col].values\n",
        "\n",
        "    # Convert labels to one-hot encodings\n",
        "    #targets = tf.keras.utils.to_categorical(df[target_col], nlabels)\n",
        "    \n",
        "    return features#, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GenLOAAsfX9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prub = pd.read_csv(os.path.join(DATA_DIRECTORY, 'test.csv'))\n",
        "prub_ds = tf.data.Dataset.from_tensor_slices(process_features2(prub)).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjd0cqS1kSAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "predict = numpy.argmax(model.predict(prub_ds), axis=1)\n",
        "\n",
        "con = pd.DataFrame(prub.PID,columns=['PID'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EZiryhFj3Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "con['AdoptionSpeed'] = predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjwZxWKrlcdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "con.set_index('PID',inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eEznz86llfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "con.to_csv('prediction.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_NpfePbnotm",
        "colab_type": "code",
        "outputId": "3a6a7c9b-71c7-4330-fa1a-1e4a8269f98b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "con.iloc[3121]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdoptionSpeed    4\n",
              "Name: 10746, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlZiv1-qIZga",
        "colab_type": "code",
        "outputId": "f93a7a0e-82b3-4008-ffc6-d2c360b37afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "seaborn.countplot(con.AdoptionSpeed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9c7a105390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEHCAYAAABbZ7oVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EMK57t_pGNN",
        "colab_type": "code",
        "outputId": "acb1ab4e-1721-41fb-84f1-3059438f62ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "prub.PID"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0           1\n",
              "1           2\n",
              "2           7\n",
              "3           9\n",
              "4          11\n",
              "        ...  \n",
              "4406    14984\n",
              "4407    14986\n",
              "4408    14987\n",
              "4409    14989\n",
              "4410    14990\n",
              "Name: PID, Length: 4411, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIxcp0Rbprzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}