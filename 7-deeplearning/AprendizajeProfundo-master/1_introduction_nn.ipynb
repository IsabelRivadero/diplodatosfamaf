{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0b02df97-cbb3-4759-9371-cbbecd0ccd86"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Construyendo una red neuronal con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "03d05899-7ff9-4413-aa67-c7a96bbdfcde"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué librerías necesitamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "fa44eec5-93b3-4e4b-adcb-1065bb5cc474"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Otras librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "79849185-892a-41ce-b4a2-26db6ad19597"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cargando los datos del MNIST\n",
    "\n",
    "- El conjunto de datos a utilizar es el **[MNIST](http://yann.lecun.com/exdb/mnist/)**.\n",
    "- Es un conjunto estándar para hacer *reconocimiento de imágenes*.\n",
    "- Buscamos entrenar un clasificador que reconozca que dígito es mostrado en la imagen.\n",
    "- El MNIST está compuesto por imágenes de 28x28 píxeles representadas como matrices.\n",
    "- La salida son 10 clases (dígitos del 0 al 9).\n",
    "- Se preprocesará para *convertir las matrices en vectores* y transformar las etiquetas en representaciones *one-of-k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # For mini-batch gradient descent\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "input_size = 28*28\n",
    "train_examples = 60000\n",
    "test_examples = 10000\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the dataset to convert the examples from 2D matrixes to 1D arrays.\n",
    "x_train = x_train.reshape(train_examples, input_size)\n",
    "x_test = x_test.reshape(test_examples, input_size)\n",
    "\n",
    "# normalize the input\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contruyendo la red neuronal\n",
    "\n",
    "- Comenzaremos por construir un *perceptrón multicapa* que es la red neuronal más común.\n",
    "- El modelo más simple en Keras es una concatenación de capas (layers).\n",
    "    - Se llama modelo secuencial.\n",
    "- La capa más básica es la *densa* (dense o fully connected).\n",
    "    - Internamente tiene dos variables: una matriz de pesos y un vector de biases. Keras nos abstrae de todo eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input to hidden layer\n",
    "model.add(Dense(512, input_shape=(input_size,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Hidden to output layer\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para imprimir una descripción del modelo existe un comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funciones de Activación\n",
    "\n",
    "- Una red neuronal con activación lineal no tiene mucho más poder de representación que un algoritmo lineal.\n",
    "- Para expresar no linearidad en la red neuronal se necesitan funciones no lineales de activación.\n",
    "- Una función de activación común es la *sigmoide* (o logística).\n",
    "- Keras soporta varias funciones de activación: rectified linea unit (ReLU), tangenge hiperbólica, sigmoide \"dura\", etc.\n",
    "    - Hoy en día, por sus propiedades, ReLU suele ser la más utilizada [1].\n",
    "- La función de activación *softmax* es utilizada al final de la red y sirve para clasificación.\n",
    "\n",
    "![Funciones de activación](images/activation_functions.png \"Funciones de activación\")\n",
    "<div style=\"text-align: right;\">Fuente: https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 52,650\n",
      "Trainable params: 52,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularización de la red\n",
    "\n",
    "### Regularización de los pesos\n",
    "\n",
    "- La red puede regularizarse penalizando los pesos.\n",
    "- Los pesos se regularizan mediante alguna norma:\n",
    "    - L1 es la suma del valor absoluto: ${\\displaystyle \\lambda \\sum_{i=1}^{k} |w_i|}$\n",
    "    - L2 es la suma del valor cuadrado, es la más común: ${\\displaystyle \\lambda \\sum_{i=1}^{k} w_i^2}$\n",
    "    - Elastic net es una combinación de ambas: ${\\displaystyle \\lambda_1 \\sum_{i=1}^{k} |w_i| + \\lambda_2 \\sum_{i=1}^{k} w_i^2}$\n",
    "- Para un análisis detallado de la diferencia entre L1 y L2 revisar [\\[2\\]](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(10, activation='softmax', kernel_regularizer=regularizers.l2(0.01))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dropout\n",
    "\n",
    "- Otra forma muy usada a la hora de regularizar es el **dropout** [3].\n",
    "- Es extremadamente efectivo y simple.\n",
    "- Es complementario a L1/L2/ElasticNet.\n",
    "- Durante el entrenamiento se implementa apagando un neurón con alguna probabilidad **_p_** (un hiperparámetro).\n",
    "\n",
    "![Dropout](images/dropout.jpeg \"Dropout\")\n",
    "<div style=\"text-align: right;\">Fuente: Trabajo de Srivastava et al. [3]</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dropout en Keras\n",
    "\n",
    "- Se aplica agregando capas al modelo.\n",
    "- Se llaman capas `Dropout` y se agrega a cada capa que se quiere regularizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch Normalization\n",
    "\n",
    "- En general, para acelerar la convergencia de la red, se normalizan los features de entrada, de manera que todos estén en un rango similar.\n",
    "- Esta idea también puede llevarse a las capas ocultas de la red.\n",
    "- La idea de la \"Normalización por Lotes\" (*Batch Normalization*) [4] es reducir el rango en el que se mueven los valores de las neuronas ocultas.\n",
    "- La manera en que se hace esto es restarle, a cada salida de cada capa oculta, la media del lote (batch) de datos de entrenamiento y dividirlo por la desviación estándar (a grandes razgos).\n",
    "- Como resultado, la red converge más rápido e incluso se genera un efecto de regularización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BatchNormalization en Keras.\n",
    "\n",
    "- Se aplica agregando capas al modelo.\n",
    "- Se llaman capas `BatchNormalization` y se agrega a cada capa que se quiere normalizar.\n",
    "- El `momentum` es un parámetro que decide cuánta información de los lotes anteriores se tiene en cuenta a la hora de normalizar el lote actual (en el trabajo original, este es de `0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    BatchNormalization(momentum=0),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparando el modelo para entrenarlo\n",
    "\n",
    "- Para minimizar una red neuronal necesitamos *calcular sus gradientes*.\n",
    "    - Esto se hace con el algoritmo de *retropropagación*.\n",
    "- Keras tiene la capacidad de hacerlo automáticamente.\n",
    "    - Esto se conoce como _diferenciación automática_ y es algo común en los frameworks de deep learning.\n",
    "- El modelo de Keras necesita *compilarse*.\n",
    "    - Recordar que lo que armamos en Keras (o TensorFlow) es un grafo de computación.\n",
    "- Al compilar el modelo los parámetros más importantes son la función de costo y el algoritmo de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Funciones de costo y algoritmos de optimización\n",
    "\n",
    "- La función de costo puede cambiar de acuerdo al tipo de problema (clasificación binaria/multiclase o regresión).\n",
    "    - La funciones más comunes son la media del error cuadrático (_mean squared error_) para regresión y la entropía cruzada (_crossentropy_) para clasificación.\n",
    "- El algoritmo de optimización es el que entrena la red. Existen varios, que en si son variaciones del algoritmo de _descenso por la gradiente_.\n",
    "\n",
    "<div style=\"text-align: center; margin: 5px 0;\">\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"images/contours_evaluation_optimizers.gif\" alt=\"Optimización\" style=\"width: 350px;\"/>\n",
    "    </div>\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"images/saddle_point_evaluation_optimizers.gif\" alt=\"Optimización\" style=\"width: 350px;\"/>\n",
    "    </div>\n",
    "</div>\n",
    "<div style=\"text-align: right;\">Fuente: http://ruder.io/optimizing-gradient-descent/</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Compilando el modelo Keras y visualizando la arquitectura\n",
    "\n",
    "- Con el método `.compile()` podemos compilar el modelo de Keras.\n",
    "- Además de la función de costo y el algoritmo de optimización se le pueden pasar métricas para llevar registro además del error de los datos (e.g. la exactitud o la precisión).\n",
    "- Una vez compilado el modelo la modificación requerirá rehacerlo desde cero (salvo que se guarden y carguen los pesos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                  # También podría ser el string \"Adagrad\" con los parámetros por defecto\n",
    "              metrics=['accuracy'])  # La métrica sirve para llevar algún registro además del costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### una vez que compilamos el modelo no podemos modificar la arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizar la arquitectura (opcional)\n",
    "\n",
    "Opcionalmente, si instalamos las librerías extras pedidas en el setup y utilizando `vis_util`, podemos visualizar el grafo de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 268.00 337.00\" width=\"268pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 264,-333 264,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140316380610512 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140316380610512</title>\n",
       "<polygon fill=\"none\" points=\"45,-292.5 45,-328.5 215,-328.5 215,-292.5 45,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"130\" y=\"-306.8\">dense_24_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140315602987944 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140315602987944</title>\n",
       "<polygon fill=\"none\" points=\"75.5,-219.5 75.5,-255.5 184.5,-255.5 184.5,-219.5 75.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"130\" y=\"-233.8\">dense_24: Dense</text>\n",
       "</g>\n",
       "<!-- 140316380610512&#45;&gt;140315602987944 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140316380610512-&gt;140315602987944</title>\n",
       "<path d=\"M130,-292.4551C130,-284.3828 130,-274.6764 130,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"133.5001,-265.5903 130,-255.5904 126.5001,-265.5904 133.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140315602985424 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140315602985424</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 260,-182.5 260,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"130\" y=\"-160.8\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140315602987944&#45;&gt;140315602985424 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140315602987944-&gt;140315602985424</title>\n",
       "<path d=\"M130,-219.4551C130,-211.3828 130,-201.6764 130,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"133.5001,-192.5903 130,-182.5904 126.5001,-192.5904 133.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140316380608272 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140316380608272</title>\n",
       "<polygon fill=\"none\" points=\"67.5,-73.5 67.5,-109.5 192.5,-109.5 192.5,-73.5 67.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"130\" y=\"-87.8\">dropout_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 140315602985424&#45;&gt;140316380608272 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140315602985424-&gt;140316380608272</title>\n",
       "<path d=\"M130,-146.4551C130,-138.3828 130,-128.6764 130,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"133.5001,-119.5903 130,-109.5904 126.5001,-119.5904 133.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140316380607488 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140316380607488</title>\n",
       "<polygon fill=\"none\" points=\"75.5,-.5 75.5,-36.5 184.5,-36.5 184.5,-.5 75.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"130\" y=\"-14.8\">dense_25: Dense</text>\n",
       "</g>\n",
       "<!-- 140316380608272&#45;&gt;140316380607488 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140316380608272-&gt;140316380607488</title>\n",
       "<path d=\"M130,-73.4551C130,-65.3828 130,-55.6764 130,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"133.5001,-46.5903 130,-36.5904 126.5001,-46.5904 133.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model, dpi=72).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenamiento\n",
    "\n",
    "- Una vez compilado el modelo, está listo para ser entrenado.\n",
    "- Keras tiene una interfaz similar a Scikit-Learn, con lo método `fit` y `predict`.\n",
    "- Para entrenar se necesitan 3 parámetros:\n",
    "    - El conjunto de datos de entrenamiento (datos y etiquetas).\n",
    "    - El tamaño del batch para hacer _mini-batch gradient descent_.\n",
    "    - La cantidad de épocas que entrenar.\n",
    "    - Eventualmente le podemos pasar datos para hacer validación del modelo.\n",
    "    - El parámetro `verbose` nos imprime información útil respecto al desempeño del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### notas: muy parecido a scikit learn, la cantidad para entrenar por defecto es uno, validation data se da como una tupla de valores, verbose viene por defecto en 1, si pusiera history= model.fit ...etc entonces me mostraria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 24s 400us/sample - loss: 2.3641 - accuracy: 0.5511 - val_loss: 1.2733 - val_accuracy: 0.8579\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 1.3131 - accuracy: 0.7952 - val_loss: 0.9294 - val_accuracy: 0.9036\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 1.0353 - accuracy: 0.8419 - val_loss: 0.7646 - val_accuracy: 0.9158\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.8711 - accuracy: 0.8649 - val_loss: 0.6648 - val_accuracy: 0.9180\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.7562 - accuracy: 0.8779 - val_loss: 0.5759 - val_accuracy: 0.9243\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.6650 - accuracy: 0.8901 - val_loss: 0.4975 - val_accuracy: 0.9319\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.5909 - accuracy: 0.8970 - val_loss: 0.4396 - val_accuracy: 0.9381\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5355 - accuracy: 0.9031 - val_loss: 0.3941 - val_accuracy: 0.9376\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4929 - accuracy: 0.9076 - val_loss: 0.3546 - val_accuracy: 0.9410\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4543 - accuracy: 0.9109 - val_loss: 0.3333 - val_accuracy: 0.9432\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=(x_test, y_test), verbose=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4245 - accuracy: 0.9151 - val_loss: 0.3046 - val_accuracy: 0.9444\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.3992 - accuracy: 0.9181 - val_loss: 0.2849 - val_accuracy: 0.9474\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.3721 - accuracy: 0.9225 - val_loss: 0.2707 - val_accuracy: 0.9500\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.3532 - accuracy: 0.9252 - val_loss: 0.2531 - val_accuracy: 0.9518\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3384 - accuracy: 0.9258 - val_loss: 0.2329 - val_accuracy: 0.9539\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.3272 - accuracy: 0.9280 - val_loss: 0.2334 - val_accuracy: 0.9531\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3114 - accuracy: 0.9293 - val_loss: 0.2226 - val_accuracy: 0.9556\n",
      "Epoch 8/10\n",
      "11008/60000 [====>.........................] - ETA: 2s - loss: 0.3025 - accuracy: 0.9320"
     ]
    }
   ],
   "source": [
    "history= model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=(x_test, y_test), verbose=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2ddc15ba6d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "- [1] LeCun, Yann, Bengio, Yoshua, and Hinton, Geoffrey. \"Deep learning.\" Nature 521, no. 7553 (2015): 436-444.\n",
    "- [2] \"Differences between L1 and L2 as Loss Function and Regularization\". http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/\n",
    "- [3] Srivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. \"Dropout: a simple way to prevent neural networks from overfitting.\" Journal of machine learning research 15, no. 1 (2014): 1929-1958. Harvard.\n",
    "- [4] Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
